# CPH 09: Производительность Ceph
##### [Оглавление](../README.md)

### Задание:
Проведите ряд тестов кластера ceph, измените параметры ядра на машинах кластера и повторите замер. Все замеры записывайте в файлы, указанные в заданиях.

##### Для выполнения задания необходимо сделать следующие шаги:

* Создайте пул rbd и диск на 10 гб. Установите параметр пула size 3 pg 8 и скопируйте ключ на клиент.
* Подготовьте rbd к тестам.
* Проведите тесты на ceph1 и запишите результаты в файл test№(1-3):
    * с кластера запись --no-cleanup 10 циклов (цель: Average IOPS) - результат в /root/test1 (rados bench).
    * с кластера чтение последовательное 10 циклов (цель: Average IOPS) - результат в /root/test2 (rados bench).
    * с кластера чтение случайное 10 циклов (цель: Average IOPS) - результат в /root/test3 (rados bench).
* Очистите пул от тестовых данных.
* На клиенте проведите тест записи стандартными блоками --io-total 1719973000 (цель: ops/sec: bytes/sec:) - вывод &> в /root/test1 (rbd bench-write).
* Смонтируйте диск в директорию /test (фс xfs), запустите тест записи dd на файл в ней bs=1G count=4 oflag=direct (цель: скорость записи в mb/sec) - вывод &> в /root/test2 (dd).
* Прочитайте созданный файл c помощью dd bs=1G count=4 iflag=direct (цель: скорость чтения в mb/sec) - вывод &> в /root/test3 (dd).
* На всех узлах кластера примените настройки:
fs.aio-max-nr=1048576 (echo 1048576 > /proc/sys/fs/aio-max-nr)
echo "8192" > /sys/block/vdb/queue/read_ahead_kb
* На кластере выставьте пулу большее количество PG до 32. Подождите минуту и повторите все измерения, занося результаты в файлы /root/newtest1, /root/newtest2, /root/newtest3.

#### Первоначальная проверка:
* Проверка наличия в ceph1 файла /root/test1 с выводом теста: FAILED
* Проверка наличия в ceph1 файла /root/test2 с выводом теста: FAILED
* Проверка наличия в ceph1 файла /root/test3 с выводом теста: FAILED
* Проверка наличия в client файла /root/test1 с выводом теста: FAILED
* Проверка наличия в client файла /root/test2 с выводом теста: FAILED
* Проверка наличия в client файла /root/test3 с выводом теста: FAILED
* Проверка на ceph1 равенства 32 значения pg_num: FAILED
* Задание Проверка aio-max-nr на всех узлах кластера настроено верно: OK
* Задание Проверка read_ahead_kb на всех узлах кластера настроено неверно: FAILED
* Проверка наличия в ceph1 файла /root/newtest1 с выводом теста: FAILED
* Проверка наличия в ceph1 файла /root/newtest2 с выводом теста: FAILED
* Проверка наличия в ceph1 файла /root/newtest3 с выводом теста: FAILED
* Проверка наличия в client файла /root/newtest1 с выводом теста: FAILED
* Проверка наличия в client файла /root/newtest2 с выводом теста: FAILED
* Проверка наличия в client файла /root/newtest3 с выводом теста: FAILED

## Практика
#### Замер производительности
Немного вводных, так как мы работаем на облаке:
1. Бессмысленно тестировать сеть между клиентом и кластером Ceph в виртуальной среде, так как они могут быть на одной физической ноде.
2. Синтетическая нагрузка, которую мы можем запустить между сетями, не предусматривает сброс чего-либо на диск, поэтому она всегда будет максимальная (iperf мы ничего не добьемся).
#### Мы смотрим на следующие показатели:

* среднюю скорость
* средние IOPS
Тестируем запись в обычном режиме. Здесь нам важен показатель Average IOPS, его фиксируем.
```bash
rados bench -p rbd 10 write --no-cleanup
```
Следующий тест покажет последовательное чтение — данные пишутся один за другим в одну сессию, а не параллельно. Также фиксируем значение Average IOPS.
```bash
rados bench -p rbd 10 seq
```
Проверяем случайное чтение — чтение с минимальными опциями, которое обычно гоняют все benchmark. Смотрим усредненные IOPS.
```bash
rados bench -p rbd 10 rand
```
Очищаем пул. Тестовые данные больше не нужны.
```bash
rados -p rbd cleanup
```
Дальше делаем тест производительности записи блочного устройства на Ceph. Для этого мы используем блочное устройство test. Здесь обращаем внимание на ops/sec и bytes/sec.
```bash
rbd bench-write test --io-total 171997300
```

Следующий тест аналогичный, только клиенте. Это тест на узле крупными блоками. Снова фиксируем ops/sec и bytes/sec.
```bash
rbd bench-write rbd/disk01  --io-total 1719973000 --io-size 4096000 --name client.rbd
```
Следующий тест приблизительно такой же, только убираем io_size и тестируем стандартными блокаими. Фиксируем ops/sec и bytes/sec.
```bash
rbd bench-write rbd/disk01  --io-total 1719973000 --name client.rbd
```
Перейдем к тесту записи с помощью dd. Попробуем записать 2 Гига (count=2). Фиксируем среднюю скорость.
```bash
dd if=/dev/zero of=/test/deleteme bs=1G count=2 oflag=direct
```
Проверим чтение того же файла. Оно будет быстрее, среднюю скорость также фиксируем.
```bash
dd if=/test/deleteme of=/dev/null bs=1G count=2 iflag=direct
```
Теперь тест fio. Fio — это генератор нагрузки и комплексный тест. Он часто используется при тестировании дисков. Мы будем использовать его с драйвером, который работает с ceph RBD. Чтобы это использовать, необходимо находиться на кластере.
```bash
cat /root/test.fio
```
Добавляем в файл root/test.fio следующие данные:
```bash
[write-4M]
description="write test with block size of 4M"
ioengine=rbd
clientname=admin
pool=rbd
rbdname=test disk
iodepth=32
runtime=120
rw=write #_write means sequential write, randwrite means random write, read means sequential read, randread means random read_
bs=4M
```
Установим утилиту fio через ```bash yum install fio```.

Запускаем сам тест (процесс сбора и обработки данных небыстрый, придется подождать), после фиксируем среднюю скорость записи:
```bash
fio /root/test.fio
```
Теперь возвращаемся на клиента. На клиенте выставим запись с низкой глубиной очереди, с высокой глубиной очереди и последовательную запись.

Глубина очереди – это количество одновременных запросов на чтение или запись, которые сервер посылает дисковой подсистеме.

Здесь используем стандартное ioengine=libaio — это библиотека доступа к блочным устройствам. Фиксируем скорость записи.
```bash
fio -ioengine=libaio -direct=1 -name=test -bs=4M -iodepth=16 -rw=write -runtime=60 -filename=/dev/rbd0
```
Если вы будете в ходе своей работы проводить подобные тестирования, то вам нужны отдельные блочные устройства, которые вы будете тестировать.

Запустим последовательную запись (фиксируем те же показатели):
```bash
fio -ioengine=libaio -direct=1 -sync=1 -name=test -bs=4k -iodepth=1 -rw=randwrite -runtime=60 -filename=/dev/rbd0
```
Теперь стандартную:
```bash
fio -ioengine=libaio -direct=1 -name=test -bs=4k -iodepth=128 -rw=randwrite -runtime=60 -filename=/dev/rbd0
```
Тюнинг
Переходим на Ceph, меняем параллелизм, увеличивая количество PG с 32 до 64.
```bash
ceph osd pool set rbd pg_num 64
ceph osd pool set rbd pgp_num 64
```
На всех устройствах кластера Ceph увеличиваем размер предчтения:
```bash
echo “8192” > /sys/block/vdb/queue/read ahead kb
```
Повторный замер производительности
Повторяем все тесты из раздела «Замер производительности».

Тюнинг может не принести весомых результатов. Мы говорили в теории, что основное — это диски, память и настройки Ceph.

Результаты сведены. Мы научились запускать тесты и просматривать их результаты. На практике рекомендуется использовать точно такой же подход. Он самый надежный. Не ленитесь «прогонять» несколько раз усредненные значения. С помощью этого Вы сможете находить «аномалии».

