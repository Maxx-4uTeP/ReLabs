# CPH 11: Резервное копирование и логирование Ceph
##### [Оглавление](../README.md)

### Задание:
Выдайте на клиенте диск с кластера, запишите на него информацию, снимите бекап с помощью снепшота ceph и восстановите данные.

Создайте и подготовьте пул rbd и диск на 1 гб. Скопируйте ключ ceph на клиент.
Примапьте и смонтируйте диск в директорию /test на клиенте.
Заполните директорию /test файлами для снепшот.
Сделайте снепшот snap01 диска на клиенте.
Удалите содержимое /test, отмонтируйте и отмапьте диск.
Восстановите снепшот snap01 и смонтируйте его в директорю /test, убедитесь, что файлы восстановились.

#### Первоначальная проверка:
Проверка пула rbd: FAILED
Проверка размера диска rbd: FAILED
Проверка наличия snapshot: FAILED


#### Практика
У нас есть узел, к которому прикреплен RBD диск в папку test. Копируем туда данные

cp –r/ etc/* /test

— и ничего не работает. Лезем на Ceph, смотрим статус, видим и понимаем, что кем-то до нас был выставлен флаг. Даже в логи лезть не надо, мы понимаем, что что-то стоит на паузе и мешает. На journalctl мы это увидим, он постоянно отображает ошибку.

Мы посмотрели логи, можно на Ceph настроить фильтрацию по дням и часам,

Теперь мы пойдем на клиента и сделаем snaphot:

rbd snap create rbd/disk01@snap01 --name client.rbd

Видим, что на файловой системе все данные лежат в папке test. Удаляем их.

Теперь запускаем восстановление snapshot. Делаем rollback:

rbd snap rollback rbd/disk01@snap01 --name client.rbd


```bash
ceph osd pool create rbd 32 32
rbd create disk01 --size 1G --pool rbd
rbd snap create rbd/disk01@snap01 --name client.rbd
rbd snap rollback rbd/disk01@snap01 --name client.rbd
```

